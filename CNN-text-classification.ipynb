{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gc\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This dataset is from Kaggle Competition, Toxic Comment Classification Challenge, \n",
    "#that train dataset contains 159571 rows and 8 columns, which are id, comment_text, \n",
    "#toxic, sever_toxic, obscene, threat, insult and identity_hate.\n",
    "#The test dataset has over 150000 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv') \n",
    "df_test = pd.read_csv('test.csv')\n",
    "train_input = df_train['comment_text']\n",
    "test_input = df_test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read the FastText Pre-trained Word Embedding in to a dictionary.\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open('./wiki.en.vec'))\n",
    "del embeddings_index['2519370'] # The first row of the file is useless, so delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2519370"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index) \n",
    "#FastText Word Embedding file contains 2500000 words including punctuations.\n",
    "#It doesn't contains 0-9 and words like I'm, can't and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "maxlen = 150 \n",
    "#Set the max length of each comment. If it is longer than 150 then cut if off,\n",
    "#if it is shorter than 150 then pad it up to 150.\n",
    "#This max length can be choosen in different ways. \n",
    "#Here it is a number that near 80 percentile of all comment length in training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data cleaning function\n",
    "def clean(string):\n",
    "    string = re.sub(r'\\n', ' ', string)\n",
    "    string = re.sub(r'\\t', ' ', string)\n",
    "    string = re.sub(\"[^A-Za-z\\(\\)\\,\\.\\?\\'\\!]\", \" \", string)\n",
    "    string = re.sub(\"\\'m\", ' am ', string)\n",
    "    string = re.sub(\"\\'s\", ' is ', string)\n",
    "    string = re.sub(\"can\\'t\", 'cannot ', string)\n",
    "    string = re.sub(\"n\\'t\", ' not ', string)\n",
    "    string = re.sub(\"\\'ve\", ' have ', string)\n",
    "    string = re.sub(\"\\'re\", ' are ', string)\n",
    "    string = re.sub(\"\\'d\", \" would \", string)\n",
    "    string = re.sub(\"\\'ll\", \" will \", string)\n",
    "    string = re.sub(\"\\,\", \" , \", string)\n",
    "    string = re.sub(\"\\'\", \" ' \", string)\n",
    "    string = re.sub(\"\\.\", \" . \", string)\n",
    "    string = re.sub(\"\\!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r'\\s{2,}', ' ', string.lower())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_input.apply(clean)\n",
    "y_train = df_train[['toxic','severe_toxic',\"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\n",
    "x_test = test_input.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After data clean there might be some record have nothing in comment_text, fill with a word.\n",
    "x_train = x_train.fillna('fillna')\n",
    "x_test = x_test.fillna('fillna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dictionary whose keys contains all words in train dataset that also shown \n",
    "#in FastText word embeddings.\n",
    "lst = []\n",
    "for line in x_train:\n",
    "    lst += line.split()\n",
    "    \n",
    "count = Counter(lst)\n",
    "for k in list(count.keys()):\n",
    "    if k not in embeddings_index:\n",
    "        del count[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133874"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = dict(sorted(count.items(), key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {k:v for (k,v) in count.items() if v >= 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79101"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = dict(zip(list(count.keys()),range(1,79101 + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = {}\n",
    "for key in count:\n",
    "    embedding_matrix[key] = embeddings_index[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create teh word embedding matrix where the first element is all zeros which is for word\n",
    "#that is not shown and the padding elements.\n",
    "W = np.zeros((1,300))\n",
    "W = np.append(W, np.array(list(embedding_matrix.values())),axis=0)\n",
    "W = W.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100001, 300)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same Step for text dataset.\n",
    "lst = []\n",
    "for line in x_test:\n",
    "    lst += line.split()\n",
    "    \n",
    "count_test = Counter(lst)\n",
    "for k in list(count_test.keys()):\n",
    "    if k not in embedding_matrix:\n",
    "        del count_test[k]\n",
    "    else:\n",
    "        count_test[k] = count[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59918"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "631"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Release memory.\n",
    "del lst\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the train dataset to be a sequence of ids of words.\n",
    "for i in range(len(x_train)):\n",
    "    temp = x_train[i].split()\n",
    "    for word in temp[:]:\n",
    "        if word not in count:\n",
    "            temp.remove(word)\n",
    "    for j in range(len(temp)):\n",
    "        temp[j] = count[temp[j]]\n",
    "    x_train[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_test)):\n",
    "    temp = x_test[i].split()\n",
    "    for word in temp[:]:\n",
    "        if word not in count_test:\n",
    "            temp.remove(word)\n",
    "    for j in range(len(temp)):\n",
    "        temp[j] = count_test[temp[j]]\n",
    "    x_test[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Create evaluation dataset.\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(x_train, y_train, train_size=0.80, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad sequence to 150 length.\n",
    "train_x = sequence.pad_sequences(list(Xtrain), maxlen = maxlen)\n",
    "val_x = sequence.pad_sequences(list(Xval), maxlen = maxlen)\n",
    "test_x = sequence.pad_sequences(list(x_test), maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save for easy loading.\n",
    "pd.DataFrame(W).to_csv('./W.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-d6dfcf9477d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings_index' is not defined"
     ]
    }
   ],
   "source": [
    "del embeddings_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file.\n",
    "pd.DataFrame(train_x).to_csv('./train_x.csv', sep = ',', index = False)\n",
    "pd.DataFrame(val_x).to_csv('./val_x.csv', sep = ',', index = False)\n",
    "pd.DataFrame(test_x).to_csv('./test_x.csv', sep = ',', index = False)\n",
    "pd.DataFrame(ytrain).to_csv('./ytrain.csv', sep = ',', index = False)\n",
    "pd.DataFrame(yval).to_csv('./yval.csv', sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_train).to_csv('./x_train.csv', sep = ',', index = False)\n",
    "pd.DataFrame(y_train).to_csv('./y_train.csv', sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runing Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gc\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = pd.read_csv('W.csv')\n",
    "W = W.set_index(W.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array(W).astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('train_x.csv')\n",
    "val_x = pd.read_csv('val_x.csv')\n",
    "test_x = pd.read_csv('./test_x.csv', sep = ',')\n",
    "ytrain = pd.read_csv('./ytrain.csv', sep = ',')\n",
    "yval = pd.read_csv('./yval.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders and CNN construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [1,2,3,4]\n",
    "num_filters = 32\n",
    "batch_size = 300\n",
    "#This large batch_size is specially for this case. Usually it is between 64-128.\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "embedding_size = 300\n",
    "sequence_length = 150\n",
    "num_epochs = 3 #Depends on your choice.\n",
    "dropout_keep_prob = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(tf.int32, [None, sequence_length], name = \"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [None,6], name = \"input_y\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_chars = tf.nn.embedding_lookup(W, input_x)\n",
    "embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(data):\n",
    "    pooled_outputs = []\n",
    "    \n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        \n",
    "        filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "        \n",
    "        w = tf.Variable(tf.truncated_normal(filter_shape,stddev = 0.05), name = \"w\")\n",
    "        b = tf.Variable(tf.truncated_normal([num_filters], stddev = 0.05), name = \"b\")\n",
    "            \n",
    "        conv = tf.nn.conv2d(\n",
    "            data,\n",
    "            w,\n",
    "            strides = [1,1,1,1],\n",
    "            padding = \"VALID\",\n",
    "            name = \"conv\"\n",
    "        )\n",
    "        h = tf.nn.relu(tf.nn.bias_add(conv, b), name = \"relu\")\n",
    "        pooled = tf.nn.max_pool(\n",
    "            h,\n",
    "            ksize = [1, sequence_length - filter_size + 1, 1, 1],\n",
    "            strides = [1,1,1,1],\n",
    "            padding = \"VALID\",\n",
    "            name = \"pool\"\n",
    "        )\n",
    "        \n",
    "        pooled_outputs.append(pooled)\n",
    "    \n",
    "    #return pooled_outputs\n",
    "    h_pool = tf.concat(pooled_outputs, 3)\n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "    return h_pool_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool_flat = CNN(embedded_chars_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the first dense layer, reduce the node to half.\n",
    "wd1 = tf.Variable(tf.truncated_normal([num_filters_total, int(num_filters_total/2)], stddev=0.05), name = \"wd1\")\n",
    "bd1 = tf.Variable(tf.truncated_normal([int(num_filters_total/2)], stddev = 0.05), name = \"bd1\")\n",
    "layer1 = tf.nn.xw_plus_b(h_drop, wd1, bd1, name = 'layer1') # Do wd1*h_drop + bd1\n",
    "layer1 = tf.nn.relu(layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second dense layer, reduce the outputs to 6.\n",
    "wd2 = tf.Variable(tf.truncated_normal([int(num_filters_total/2),6], stddev = 0.05), name = 'wd2')\n",
    "bd2 = tf.Variable(tf.truncated_normal([6], stddev = 0.05), name = \"bd2\")\n",
    "layer2 = tf.nn.xw_plus_b(layer1, wd2, bd2, name = 'layer2') \n",
    "prediction = tf.nn.sigmoid(layer2)# Make it to be 0-1.\n",
    "#pred_clipped = tf.clip_by_value(prediction, 1e-10, 0.9999999) \n",
    "#For some special loss function clip is necessary. Like log(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = layer2, labels = input_y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.0001).minimize(loss)\n",
    "#Learning rates usually is small for CNN compared with pure neural network. \n",
    "#Need to define a approriate learning rate before you run on the whole dataset.\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(prediction), input_y), tf.float32))\n",
    "#correct_prediction = tf.equal(tf.argmax(input_y, 1), tf.argmax(prediction, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocks and Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define batch generation function.\n",
    "def generate_batch(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    l = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        l += 1\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Test data. Can use generate_batch function.\n",
    "def blocks(data, block_size):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    nums = int((data_size-1)/block_size) + 1\n",
    "    for block_num in range(nums):\n",
    "        if block_num == 0:\n",
    "            print(\"prediction start!\")\n",
    "        start_index = block_num * block_size\n",
    "        end_index = min((block_num + 1) * block_size, data_size)\n",
    "        print(end_index)\n",
    "        yield data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ready to predict test data.\n",
    "x_train = pd.DataFrame(train_x).append(pd.DataFrame(val_x))\n",
    "y_train = pd.DataFrame(ytrain).append(pd.DataFrame(yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 150)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 6)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason to create 7 different batches here is because \n",
    "#I want to make the data totally shuffled to reduce the risk that one batch have all 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch1 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch2 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch3 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch4 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch5 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch6 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch7 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127656, 150)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = generate_batch(list(zip(np.array(train_x), ytrain['toxic'], ytrain['severe_toxic'], ytrain['obscene'], ytrain['threat'], ytrain['insult'], ytrain['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch2 = generate_batch(list(zip(np.array(train_x), ytrain['toxic'], ytrain['severe_toxic'], ytrain['obscene'], ytrain['threat'], ytrain['insult'], ytrain['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch3 = generate_batch(list(zip(np.array(train_x), ytrain['toxic'], ytrain['severe_toxic'], ytrain['obscene'], ytrain['threat'], ytrain['insult'], ytrain['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch4 = generate_batch(list(zip(np.array(train_x), ytrain['toxic'], ytrain['severe_toxic'], ytrain['obscene'], ytrain['threat'], ytrain['insult'], ytrain['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch5 = generate_batch(list(zip(np.array(train_x), ytrain['toxic'], ytrain['severe_toxic'], ytrain['obscene'], ytrain['threat'], ytrain['insult'], ytrain['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch6 = generate_batch(list(zip(np.array(train_x), ytrain['toxic'], ytrain['severe_toxic'], ytrain['obscene'], ytrain['threat'], ytrain['insult'], ytrain['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_blocks = blocks(list(zip(np.array(val_x), yval['toxic'], yval['severe_toxic'], yval['obscene'], yval['threat'], yval['insult'], yval['identity_hate'])), 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_blocks = blocks(list(np.array(test_x)), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_bag = [batch1,batch2,batch3,batch4,batch5,batch6]#,batch7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((159571-1)/300) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 start!\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    i = 0\n",
    "    for batches in batch_bag:\n",
    "        i += 1\n",
    "        print('Epoch: ' + str(i) + ' start!')\n",
    "        avg_acc = 0\n",
    "        avg_loss = 0\n",
    "        for batch in batches:\n",
    "            batch = pd.DataFrame(batch, columns = ['a','b','c','d','e','g','f'])\n",
    "            x_batch = pd.DataFrame(list(batch['a']))\n",
    "            y_batch = batch.loc[:, batch.columns != 'a']\n",
    "            _,c, acc = sess.run([optimizer, loss, accuracy],feed_dict = {input_x: x_batch, input_y: y_batch})\n",
    "            avg_loss += c\n",
    "            avg_acc += acc\n",
    "            #print(str(c) + ' and ' + str(acc))\n",
    "            #print('pred_train')\n",
    "            #print(prediction.eval({input_x: x_batch, input_y: y_batch}))\n",
    "        avg_loss = avg_loss/532\n",
    "        avg_acc = avg_acc/532\n",
    "        print('Epoch:' + str(i) + ' loss is ' + str(avg_loss) + ', accuracy is ' + str(avg_acc))\n",
    "        #print('Evaluation Accuracy: ')\n",
    "        #print(accuracy.eval({input_x: val_x, input_y: yval}))\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for block in test_blocks:\n",
    "        block = pd.DataFrame(block)\n",
    "        pred = sess.run(prediction, feed_dict = {input_x: block})\n",
    "        df = df.append(pd.DataFrame(pred))\n",
    "    \n",
    "    print('Finish!')\n",
    "    #with open(file, 'w') as output:  \n",
    "    #    writer = csv.writer(output, lineterminator='\\n')\n",
    "    #    writer.writerow(['toxic','severe_toxic','obscene','threat','insult','identity_hate'])\n",
    "            \n",
    "    #    writer.writerows(pred)\n",
    "            \n",
    "        #c = sess.run([accuracy], feed_dict = {input_x: x_block, input_y: y_block})\n",
    "        #avg_acc += c\n",
    "        #print('evaluate accuracy: ' + str(c))\n",
    "     \n",
    "    #print('Average Accuracy: ' + str(avg_acc/107))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.096348\n",
       "1    0.000020\n",
       "2    0.060687\n",
       "3    0.000007\n",
       "4    0.046852\n",
       "5    0.000000\n",
       "dtype: float32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.round().mean() #Epoch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            0.220835\n",
       "severe_toxic     0.007822\n",
       "obscene          0.130749\n",
       "threat           0.003728\n",
       "insult           0.097092\n",
       "identity_hate    0.015036\n",
       "dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.loc[:,sub.columns != 'id'].round().mean() #Results from keras epoch = 3. 98.20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
