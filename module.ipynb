{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "raw_input = df['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for num in range(len(raw_input)):\n",
    "    temp = nltk.sent_tokenize(raw_input[num])\n",
    "    for j in range(len(temp)):\n",
    "        txt = re.sub('[^\\w\\s\\']|\\d+','',temp[j])\n",
    "        temp[j] = re.sub(r'\\n|\\s{2,}',' ',txt)\n",
    "        sentences.append(temp[j].lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = Word2Vec(sentences, iter=20, min_count=10, size=300, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('./word2vec.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"fuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('./word2vec.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develope word series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "freq = []\n",
    "for w in vocab:\n",
    "    words.append(w)\n",
    "    freq.append(vocab[w].count)\n",
    "    \n",
    "wordseries = pd.DataFrame({'word': words, 'freq': freq})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordseries = wordseries.sort_values(['freq'], ascending = [0])\n",
    "wordseries['id'] = range(1,wordseries.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsequence = dict(zip(wordseries['word'],wordseries['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "W = np.zeros((1,300))\n",
    "W = np.append(W, model[wordsequence.keys()],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            print(end_index)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [2,3,4,5]\n",
    "num_filters = 2\n",
    "batch_size = 200\n",
    "embedding_size = 300\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "sequence_length = 1403\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(tf.int32, [None, sequence_length], name = \"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [None,6], name = \"input_y\")\n",
    "dropout_keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_chars = tf.nn.embedding_lookup(W, input_x)\n",
    "embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_outputs = []\n",
    "    \n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "        \n",
    "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "        \n",
    "    w = tf.Variable(tf.truncated_normal(filter_shape,stddev = 0.1), name = \"w\")\n",
    "    b = tf.Variable(tf.truncated_normal([num_filters]), name = \"b\")\n",
    "            \n",
    "    conv = tf.nn.conv2d(\n",
    "        embedded_chars_expanded,\n",
    "        w,\n",
    "        strides = [1,1,1,1],\n",
    "        padding = \"VALID\",\n",
    "        name = \"conv\"\n",
    "    )\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, b), name = \"relu\")\n",
    "    pooled = tf.nn.max_pool(\n",
    "        h,\n",
    "        ksize = [1,sequence_length - filter_size + 1, 1, 1],\n",
    "        strides = [1,1,1,1],\n",
    "        padding = \"VALID\",\n",
    "        name = \"pool\"\n",
    "    )\n",
    "    pooled_outputs.append(pooled)\n",
    "    \n",
    "h_pool = tf.concat(pooled_outputs, 3)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd1 = tf.Variable(tf.truncated_normal([num_filters_total, int(num_filters_total/2)], stddev=0.1), name = \"wd1\")\n",
    "bd1 = tf.Variable(tf.truncated_normal([int(num_filters_total/2)]), name = \"bd1\")\n",
    "layer1 = tf.nn.xw_plus_b(h_drop, wd1, bd1, name = 'layer1')\n",
    "layer1 = tf.nn.relu(layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd2 = tf.Variable(tf.truncated_normal([int(num_filters_total/2),6]), name = 'wd2')\n",
    "bd2 = tf.Variable(tf.truncated_normal([6]), name = \"bd2\")\n",
    "layer2 = tf.nn.xw_plus_b(layer1, wd2, bd2, name = 'layer2')\n",
    "prediction = tf.nn.softmax(layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = layer2, labels = input_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.5).minimize(loss)\n",
    "#correct_prediction = tf.equal(tf.argmax(input_y, 1), tf.argmax(prediction, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_input = df_test['comment_text']\n",
    "for i in range(len(test_input)):\n",
    "    txt = re.sub('[^\\w\\s\\']|\\d+','',test_input[i])\n",
    "    txt = re.sub(r'\\n|\\s{2,}',' ',txt.lower())\n",
    "    lst = txt.split()\n",
    "    temp = []\n",
    "    for word in lst:\n",
    "        if word not in vocab:\n",
    "            temp.append(0)\n",
    "        else:\n",
    "            temp.append(wordsequence[word])\n",
    "\n",
    "    test_input[i] = temp\n",
    "test_input = sequence.pad_sequences(test_input, maxlen = sequence_length)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data split to blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 1403)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocks(data, block_size):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    nums = int((data_size-1)/block_size) + 1\n",
    "    for block_num in range(nums):\n",
    "        if block_num == 0:\n",
    "            print(\"prediction start!\")\n",
    "        start_index = block_num * block_size\n",
    "        end_index = min((block_num + 1) * block_size, data_size)\n",
    "        print(end_index)\n",
    "        yield data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_blocks = blocks(test_input,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = generate_batch(list(zip(raw_input, df['toxic'], df['severe_toxic'], df['obscene'], df['threat'], df['insult'], df['identity_hate'])), batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n",
      "8800\n",
      "9000\n",
      "9200\n",
      "9400\n",
      "9600\n",
      "9800\n",
      "10000\n",
      "10200\n",
      "10400\n",
      "10600\n",
      "10800\n",
      "11000\n",
      "11200\n",
      "11400\n",
      "11600\n",
      "11800\n",
      "12000\n",
      "12200\n",
      "12400\n",
      "12600\n",
      "12800\n",
      "13000\n",
      "13200\n",
      "13400\n",
      "13600\n",
      "13800\n",
      "14000\n",
      "14200\n",
      "14400\n",
      "14600\n",
      "14800\n",
      "15000\n",
      "15200\n",
      "15400\n",
      "15600\n",
      "15800\n",
      "16000\n",
      "16200\n",
      "16400\n",
      "16600\n",
      "16800\n",
      "17000\n",
      "17200\n",
      "17400\n",
      "17600\n",
      "17800\n",
      "18000\n",
      "18200\n",
      "18400\n",
      "18600\n",
      "18800\n",
      "19000\n",
      "19200\n",
      "19400\n",
      "19600\n",
      "19800\n",
      "20000\n",
      "20200\n",
      "20400\n",
      "20600\n",
      "20800\n",
      "21000\n",
      "21200\n",
      "21400\n",
      "21600\n",
      "21800\n",
      "22000\n",
      "22200\n",
      "22400\n",
      "22600\n",
      "22800\n",
      "23000\n",
      "23200\n",
      "23400\n",
      "23600\n",
      "23800\n",
      "24000\n",
      "24200\n",
      "24400\n",
      "24600\n",
      "24800\n",
      "25000\n",
      "25200\n",
      "25400\n",
      "25600\n",
      "25800\n",
      "26000\n",
      "26200\n",
      "26400\n",
      "26600\n",
      "26800\n",
      "27000\n",
      "27200\n",
      "27400\n",
      "27600\n",
      "27800\n",
      "28000\n",
      "28200\n",
      "28400\n",
      "28600\n",
      "28800\n",
      "29000\n",
      "29200\n",
      "29400\n",
      "29600\n",
      "29800\n",
      "30000\n",
      "30200\n",
      "30400\n",
      "30600\n",
      "30800\n",
      "31000\n",
      "31200\n",
      "31400\n",
      "31600\n",
      "31800\n",
      "32000\n",
      "32200\n",
      "32400\n",
      "32600\n",
      "32800\n",
      "33000\n",
      "33200\n",
      "33400\n",
      "33600\n",
      "33800\n",
      "34000\n",
      "34200\n",
      "34400\n",
      "34600\n",
      "34800\n",
      "35000\n",
      "35200\n",
      "35400\n",
      "35600\n",
      "35800\n",
      "36000\n",
      "36200\n",
      "36400\n",
      "36600\n",
      "36800\n",
      "37000\n",
      "37200\n",
      "37400\n",
      "37600\n",
      "37800\n",
      "38000\n",
      "38200\n",
      "38400\n",
      "38600\n",
      "38800\n",
      "39000\n",
      "39200\n",
      "39400\n",
      "39600\n",
      "39800\n",
      "40000\n",
      "40200\n",
      "40400\n",
      "40600\n",
      "40800\n",
      "41000\n",
      "41200\n",
      "41400\n",
      "41600\n",
      "41800\n",
      "42000\n",
      "42200\n",
      "42400\n",
      "42600\n",
      "42800\n",
      "43000\n",
      "43200\n",
      "43400\n",
      "43600\n",
      "43800\n",
      "44000\n",
      "44200\n",
      "44400\n",
      "44600\n",
      "44800\n",
      "45000\n",
      "45200\n",
      "45400\n",
      "45600\n",
      "45800\n",
      "46000\n",
      "46200\n",
      "46400\n",
      "46600\n",
      "46800\n",
      "47000\n",
      "47200\n",
      "47400\n",
      "47600\n",
      "47800\n",
      "48000\n",
      "48200\n",
      "48400\n",
      "48600\n",
      "48800\n",
      "49000\n",
      "49200\n",
      "49400\n",
      "49600\n",
      "49800\n",
      "50000\n",
      "50200\n",
      "50400\n",
      "50600\n",
      "50800\n",
      "51000\n",
      "51200\n",
      "51400\n",
      "51600\n",
      "51800\n",
      "52000\n",
      "52200\n",
      "52400\n",
      "52600\n",
      "52800\n",
      "53000\n",
      "53200\n",
      "53400\n",
      "53600\n",
      "53800\n",
      "54000\n",
      "54200\n",
      "54400\n",
      "54600\n",
      "54800\n",
      "55000\n",
      "55200\n",
      "55400\n",
      "55600\n",
      "55800\n",
      "56000\n",
      "56200\n",
      "56400\n",
      "56600\n",
      "56800\n",
      "57000\n",
      "57200\n",
      "57400\n",
      "57600\n",
      "57800\n",
      "58000\n",
      "58200\n",
      "58400\n",
      "58600\n",
      "58800\n",
      "59000\n",
      "59200\n",
      "59400\n",
      "59600\n",
      "59800\n",
      "60000\n",
      "60200\n",
      "60400\n",
      "60600\n",
      "60800\n",
      "61000\n",
      "61200\n",
      "61400\n",
      "61600\n",
      "61800\n",
      "62000\n",
      "62200\n",
      "62400\n",
      "62600\n",
      "62800\n",
      "63000\n",
      "63200\n",
      "63400\n",
      "63600\n",
      "63800\n",
      "64000\n",
      "64200\n",
      "64400\n",
      "64600\n",
      "64800\n",
      "65000\n",
      "65200\n",
      "65400\n",
      "65600\n",
      "65800\n",
      "66000\n",
      "66200\n",
      "66400\n",
      "66600\n",
      "66800\n",
      "67000\n",
      "67200\n",
      "67400\n",
      "67600\n",
      "67800\n",
      "68000\n",
      "68200\n",
      "68400\n",
      "68600\n",
      "68800\n",
      "69000\n",
      "69200\n",
      "69400\n",
      "69600\n",
      "69800\n",
      "70000\n",
      "70200\n",
      "70400\n",
      "70600\n",
      "70800\n",
      "71000\n",
      "71200\n",
      "71400\n",
      "71600\n",
      "71800\n",
      "72000\n",
      "72200\n",
      "72400\n",
      "72600\n",
      "72800\n",
      "73000\n",
      "73200\n",
      "73400\n",
      "73600\n",
      "73800\n",
      "74000\n",
      "74200\n",
      "74400\n",
      "74600\n",
      "74800\n",
      "75000\n",
      "75200\n",
      "75400\n",
      "75600\n",
      "75800\n",
      "76000\n",
      "76200\n",
      "76400\n",
      "76600\n",
      "76800\n",
      "77000\n",
      "77200\n",
      "77400\n",
      "77600\n",
      "77800\n",
      "78000\n",
      "78200\n",
      "78400\n",
      "78600\n",
      "78800\n",
      "79000\n",
      "79200\n",
      "79400\n",
      "79600\n",
      "79800\n",
      "80000\n",
      "80200\n",
      "80400\n",
      "80600\n",
      "80800\n",
      "81000\n",
      "81200\n",
      "81400\n",
      "81600\n",
      "81800\n",
      "82000\n",
      "82200\n",
      "82400\n",
      "82600\n",
      "82800\n",
      "83000\n",
      "83200\n",
      "83400\n",
      "83600\n",
      "83800\n",
      "84000\n",
      "84200\n",
      "84400\n",
      "84600\n",
      "84800\n",
      "85000\n",
      "85200\n",
      "85400\n",
      "85600\n",
      "85800\n",
      "86000\n",
      "86200\n",
      "86400\n",
      "86600\n",
      "86800\n",
      "87000\n",
      "87200\n",
      "87400\n",
      "87600\n",
      "87800\n",
      "88000\n",
      "88200\n",
      "88400\n",
      "88600\n",
      "88800\n",
      "89000\n",
      "89200\n",
      "89400\n",
      "89600\n",
      "89800\n",
      "90000\n",
      "90200\n",
      "90400\n",
      "90600\n",
      "90800\n",
      "91000\n",
      "91200\n",
      "91400\n",
      "91600\n",
      "91800\n",
      "92000\n",
      "92200\n",
      "92400\n",
      "92600\n",
      "92800\n",
      "93000\n",
      "93200\n",
      "93400\n",
      "93600\n",
      "93800\n",
      "94000\n",
      "94200\n",
      "94400\n",
      "94600\n",
      "94800\n",
      "95000\n",
      "95200\n",
      "95400\n",
      "95600\n",
      "95800\n",
      "96000\n",
      "96200\n",
      "96400\n",
      "96600\n",
      "96800\n",
      "97000\n",
      "97200\n",
      "97400\n",
      "97600\n",
      "97800\n",
      "98000\n",
      "98200\n",
      "98400\n",
      "98600\n",
      "98800\n",
      "99000\n",
      "99200\n",
      "99400\n",
      "99600\n",
      "99800\n",
      "100000\n",
      "100200\n",
      "100400\n",
      "100600\n",
      "100800\n",
      "101000\n",
      "101200\n",
      "101400\n",
      "101600\n",
      "101800\n",
      "102000\n",
      "102200\n",
      "102400\n",
      "102600\n",
      "102800\n",
      "103000\n",
      "103200\n",
      "103400\n",
      "103600\n",
      "103800\n",
      "104000\n",
      "104200\n",
      "104400\n",
      "104600\n",
      "104800\n",
      "105000\n",
      "105200\n",
      "105400\n",
      "105600\n",
      "105800\n",
      "106000\n",
      "106200\n",
      "106400\n",
      "106600\n",
      "106800\n",
      "107000\n",
      "107200\n",
      "107400\n",
      "107600\n",
      "107800\n",
      "108000\n",
      "108200\n",
      "108400\n",
      "108600\n",
      "108800\n",
      "109000\n",
      "109200\n",
      "109400\n",
      "109600\n",
      "109800\n",
      "110000\n",
      "110200\n",
      "110400\n",
      "110600\n",
      "110800\n",
      "111000\n",
      "111200\n",
      "111400\n",
      "111600\n",
      "111800\n",
      "112000\n",
      "112200\n",
      "112400\n",
      "112600\n",
      "112800\n",
      "113000\n",
      "113200\n",
      "113400\n",
      "113600\n",
      "113800\n",
      "114000\n",
      "114200\n",
      "114400\n",
      "114600\n",
      "114800\n",
      "115000\n",
      "115200\n",
      "115400\n",
      "115600\n",
      "115800\n",
      "116000\n",
      "116200\n",
      "116400\n",
      "116600\n",
      "116800\n",
      "117000\n",
      "117200\n",
      "117400\n",
      "117600\n",
      "117800\n",
      "118000\n",
      "118200\n",
      "118400\n",
      "118600\n",
      "118800\n",
      "119000\n",
      "119200\n",
      "119400\n",
      "119600\n",
      "119800\n",
      "120000\n",
      "120200\n",
      "120400\n",
      "120600\n",
      "120800\n",
      "121000\n",
      "121200\n",
      "121400\n",
      "121600\n",
      "121800\n",
      "122000\n",
      "122200\n",
      "122400\n",
      "122600\n",
      "122800\n",
      "123000\n",
      "123200\n",
      "123400\n",
      "123600\n",
      "123800\n",
      "124000\n",
      "124200\n",
      "124400\n",
      "124600\n",
      "124800\n",
      "125000\n",
      "125200\n",
      "125400\n",
      "125600\n",
      "125800\n",
      "126000\n",
      "126200\n",
      "126400\n",
      "126600\n",
      "126800\n",
      "127000\n",
      "127200\n",
      "127400\n",
      "127600\n",
      "127800\n",
      "128000\n",
      "128200\n",
      "128400\n",
      "128600\n",
      "128800\n",
      "129000\n",
      "129200\n",
      "129400\n",
      "129600\n",
      "129800\n",
      "130000\n",
      "130200\n",
      "130400\n",
      "130600\n",
      "130800\n",
      "131000\n",
      "131200\n",
      "131400\n",
      "131600\n",
      "131800\n",
      "132000\n",
      "132200\n",
      "132400\n",
      "132600\n",
      "132800\n",
      "133000\n",
      "133200\n",
      "133400\n",
      "133600\n",
      "133800\n",
      "134000\n",
      "134200\n",
      "134400\n",
      "134600\n",
      "134800\n",
      "135000\n",
      "135200\n",
      "135400\n",
      "135600\n",
      "135800\n",
      "136000\n",
      "136200\n",
      "136400\n",
      "136600\n",
      "136800\n",
      "137000\n",
      "137200\n",
      "137400\n",
      "137600\n",
      "137800\n",
      "138000\n",
      "138200\n",
      "138400\n",
      "138600\n",
      "138800\n",
      "139000\n",
      "139200\n",
      "139400\n",
      "139600\n",
      "139800\n",
      "140000\n",
      "140200\n",
      "140400\n",
      "140600\n",
      "140800\n",
      "141000\n",
      "141200\n",
      "141400\n",
      "141600\n",
      "141800\n",
      "142000\n",
      "142200\n",
      "142400\n",
      "142600\n",
      "142800\n",
      "143000\n",
      "143200\n",
      "143400\n",
      "143600\n",
      "143800\n",
      "144000\n",
      "144200\n",
      "144400\n",
      "144600\n",
      "144800\n",
      "145000\n",
      "145200\n",
      "145400\n",
      "145600\n",
      "145800\n",
      "146000\n",
      "146200\n",
      "146400\n",
      "146600\n",
      "146800\n",
      "147000\n",
      "147200\n",
      "147400\n",
      "147600\n",
      "147800\n",
      "148000\n",
      "148200\n",
      "148400\n",
      "148600\n",
      "148800\n",
      "149000\n",
      "149200\n",
      "149400\n",
      "149600\n",
      "149800\n",
      "150000\n",
      "150200\n",
      "150400\n",
      "150600\n",
      "150800\n",
      "151000\n",
      "151200\n",
      "151400\n",
      "151600\n",
      "151800\n",
      "152000\n",
      "152200\n",
      "152400\n",
      "152600\n",
      "152800\n",
      "153000\n",
      "153200\n",
      "153400\n",
      "153600\n",
      "153800\n",
      "154000\n",
      "154200\n",
      "154400\n",
      "154600\n",
      "154800\n",
      "155000\n",
      "155200\n",
      "155400\n",
      "155600\n",
      "155800\n",
      "156000\n",
      "156200\n",
      "156400\n",
      "156600\n",
      "156800\n",
      "157000\n",
      "157200\n",
      "157400\n",
      "157600\n",
      "157800\n",
      "158000\n",
      "158200\n",
      "158400\n",
      "158600\n",
      "158800\n",
      "159000\n",
      "159200\n",
      "159400\n",
      "159571\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch = pd.DataFrame(batch, columns = ['a','b','c','d','e','f','g'])\n",
    "        x_batch = batch['a']\n",
    "        y_batch = batch.loc[:, batch.columns != 'a']\n",
    "        for i in range(len(x_batch)):\n",
    "            txt = re.sub('[^\\w\\s\\']|\\d+','',x_batch[i])\n",
    "            txt = re.sub(r'\\n|\\s{2,}',' ',txt.lower())\n",
    "            lst = txt.split()\n",
    "            temp = []\n",
    "            for word in lst:\n",
    "                if word not in vocab:\n",
    "                    temp.append(0)\n",
    "                else:\n",
    "                    temp.append(wordsequence[word])\n",
    "            x_batch[i] = temp\n",
    "        x_batch = sequence.pad_sequences(x_batch, maxlen=sequence_length)\n",
    "        #y_batch = np.array(y_batch).reshape(batch_size,6)\n",
    "        _,c = sess.run([optimizer, loss],feed_dict = {input_x: x_batch, input_y: y_batch})\n",
    "        \n",
    "        with open('csvfile.csv', \"w\") as output:\n",
    "            writer = csv.writer(output, lineterminator='\\n')\n",
    "            writer.writerow(['toxic','severe_toxic','obscene','threat','insult','identity_hate'])\n",
    "            for block in test_blocks:\n",
    "                pred = sess.run(prediction, feed_dict={input_x: block})\n",
    "                writer.writerows(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                       id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "5       00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...   \n",
       "6       0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "7       00031b1e95af7921  Your vandalism to the Matt Shirvington article...   \n",
       "8       00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...   \n",
       "9       00040093b2687caa  alignment on this subject and which are contra...   \n",
       "10      0005300084f90edc  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...   \n",
       "11      00054a5e18b50dd4  bbq \\n\\nbe a man and lets discuss it-maybe ove...   \n",
       "12      0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
       "13      0006f16e4e9f292e  Before you start throwing accusations and warn...   \n",
       "14      00070ef96486d6f9  Oh, and the girl above started her arguments w...   \n",
       "15      00078f8ce7eb276d  \"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...   \n",
       "16      0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
       "17      000897889268bc93   REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski   \n",
       "18      0009801bd85e5806  The Mitsurugi point made no sense - why not ar...   \n",
       "19      0009eaea3325de8c  Don't mean to bother you \\n\\nI see that you're...   \n",
       "20      000b08c464718505  \"\\n\\n Regarding your recent edits \\n\\nOnce aga...   \n",
       "21      000bfd0867774845  \"\\nGood to know. About me, yeah, I'm studying ...   \n",
       "22      000c0dfd995809fa  \"\\n\\n Snowflakes are NOT always symmetrical! \\...   \n",
       "23      000c6a3f0cd3ba8e  \"\\n\\n The Signpost: 24 September 2012 \\n\\n Rea...   \n",
       "24      000cfee90f50d471  \"\\n\\nRe-considering 1st paragraph edit?\\nI don...   \n",
       "25      000eefc67a2c930f  Radial symmetry \\n\\nSeveral now extinct lineag...   \n",
       "26      000f35deef84dc4a  There's no need to apologize. A Wikipedia arti...   \n",
       "27      000ffab30195c5e1  Yes, because the mother of the child in the ca...   \n",
       "28      0010307a3a50a353  \"\\nOk. But it will take a bit of work but I ca...   \n",
       "29      0010833a96e1f886  \"== A barnstar for you! ==\\n\\n  The Real Life ...   \n",
       "...                  ...                                                ...   \n",
       "159541  ffa33d3122b599d6  Your absurd edits \\n\\nYour absurd edits on gre...   \n",
       "159542  ffa95244f261527f  maybe he's got better things to do than spend ...   \n",
       "159543  ffad104337fe9891  scrap that, it does meet criteria and its gone...   \n",
       "159544  ffaed63c487a2b42                                You could do worse.   \n",
       "159545  ffb268f37788a011  , 7 March 2011 (UTC)\\nAre you also User:Bmatts...   \n",
       "159546  ffb47123b2d82762  \"\\n\\nHey listen don't you ever!!!! Delete my e...   \n",
       "159547  ffb7b4c3d3ae5842                     Thank you very, very much.  ·✆   \n",
       "159548  ffb93b0a0a1e78f9                        Talkback: 15 September 2012   \n",
       "159549  ffb998f9749bd83e                         2005 (UTC)\\n 06:35, 31 Mar   \n",
       "159550  ffba5332d6b8fd14  i agree/ on another note lil wayne is a talent...   \n",
       "159551  ffbc2db4225258dd  While about half the references are from BYU-I...   \n",
       "159552  ffbcd64a71775e04  Prague Spring \\n\\nI think that Prague Spring d...   \n",
       "159553  ffbd331a3aa269b9  I see this as having been merged; undoing one ...   \n",
       "159554  ffbdbb0483ed0841  and i'm going to keep posting the stuff u dele...   \n",
       "159555  ffc2f409658571f1  \"\\n\\nHow come when you download that MP3 it's ...   \n",
       "159556  ffc671f2acdd80e1  I'll be on IRC, too, if you have a more specif...   \n",
       "159557  ffc7bbb177c3c966  It is my opinion that that happens to be off-t...   \n",
       "159558  ffca1e81aefc48ac  Please stop removing content from Wikipedia; i...   \n",
       "159559  ffca8d71d71a3fae  Image:Barack-obama-mother.jpg listed for delet...   \n",
       "159560  ffcdcb71854f6d8a  \"Editing of article without Consensus & Remova...   \n",
       "159561  ffd2e85b07b3c7e4  \"\\nNo he did not, read it again (I would have ...   \n",
       "159562  ffd72e9766c09c97  \"\\n Auto guides and the motoring press are not...   \n",
       "159563  ffe029a7c79dc7fe  \"\\nplease identify what part of BLP applies be...   \n",
       "159564  ffe897e7f7182c90  Catalan independentism is the social movement ...   \n",
       "159565  ffe8b9316245be30  The numbers in parentheses are the additional ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "5           0             0        0       0       0              0  \n",
       "6           1             1        1       0       1              0  \n",
       "7           0             0        0       0       0              0  \n",
       "8           0             0        0       0       0              0  \n",
       "9           0             0        0       0       0              0  \n",
       "10          0             0        0       0       0              0  \n",
       "11          0             0        0       0       0              0  \n",
       "12          1             0        0       0       0              0  \n",
       "13          0             0        0       0       0              0  \n",
       "14          0             0        0       0       0              0  \n",
       "15          0             0        0       0       0              0  \n",
       "16          1             0        0       0       0              0  \n",
       "17          0             0        0       0       0              0  \n",
       "18          0             0        0       0       0              0  \n",
       "19          0             0        0       0       0              0  \n",
       "20          0             0        0       0       0              0  \n",
       "21          0             0        0       0       0              0  \n",
       "22          0             0        0       0       0              0  \n",
       "23          0             0        0       0       0              0  \n",
       "24          0             0        0       0       0              0  \n",
       "25          0             0        0       0       0              0  \n",
       "26          0             0        0       0       0              0  \n",
       "27          0             0        0       0       0              0  \n",
       "28          0             0        0       0       0              0  \n",
       "29          0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159541      1             0        1       0       1              0  \n",
       "159542      0             0        0       0       0              0  \n",
       "159543      0             0        0       0       0              0  \n",
       "159544      0             0        0       0       0              0  \n",
       "159545      0             0        0       0       0              0  \n",
       "159546      1             0        0       0       1              0  \n",
       "159547      0             0        0       0       0              0  \n",
       "159548      0             0        0       0       0              0  \n",
       "159549      0             0        0       0       0              0  \n",
       "159550      0             0        0       0       0              0  \n",
       "159551      0             0        0       0       0              0  \n",
       "159552      0             0        0       0       0              0  \n",
       "159553      0             0        0       0       0              0  \n",
       "159554      1             0        1       0       1              0  \n",
       "159555      0             0        0       0       0              0  \n",
       "159556      0             0        0       0       0              0  \n",
       "159557      0             0        0       0       0              0  \n",
       "159558      0             0        0       0       0              0  \n",
       "159559      0             0        0       0       0              0  \n",
       "159560      0             0        0       0       0              0  \n",
       "159561      0             0        0       0       0              0  \n",
       "159562      0             0        0       0       0              0  \n",
       "159563      0             0        0       0       0              0  \n",
       "159564      0             0        0       0       0              0  \n",
       "159565      0             0        0       0       0              0  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 8 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "for num in range(len(raw_input)):\n",
    "    txt = re.sub('[^\\w\\s\\']|\\d+','',raw_input[num])\n",
    "    txt = re.sub(r'\\n|\\s{2,}',' ',txt)\n",
    "    raw_input[num] = txt.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
